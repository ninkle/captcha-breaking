{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from time import clock\n",
    "from PIL import Image, ImageStat\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To mitigate space/time complexity, we'll work instead with the Tiny Imagenet dataset, which is a subset of the Imagenet dataset.  \n",
    "\n",
    "The Tiny Imagenet dataset is stored inside the directory `tiny-imagenet-200`, which itself contains 3 subdirectories: `train`, `val`, and `test`. Inside each of these subdirectories is a set of 200 folders corresponding to the 200 classes inside Tiny Imagenet.   \n",
    "\n",
    "The folders are labeled by the Wordnet synset (e.g. `n01910747`) representing that class (e.g. [\"Jellyfish\", \"cnidarian\", \"Coelenterate\"]). Inside `train`, each class contains 500 sample images while inside each of `val` and `test`, each class contains 50 samples images.\n",
    "\n",
    "Each image size in Tiny Imagenet is a 64x64x3 resolution image, which we artificially englarged to 224x224x3 in order to preserve the architectural integrity of Alexnet (256x256x3 images from Imagenet had 224x224x3 patches extracted from the original image before being fed into the network)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to combat overfitting, Krizhevsky et. al made efforts to artifically enlarge Imagenet with label-preserving transformations, namely:\n",
    "\n",
    "1. **Image Translations/Horizontal Reflections**: From each image, several random 224x224 patches were extracted, with a horizontal reflection being generated from each patch. This technique enlarged the training dataset by a factor of 2048 (significantly combatting the overfitting problem). At test time, 5 patches were taken (from the image's four corners and the centre of the image), with a reflection generated for each patch. The final prediction was the average of the network's predictions on each of these 10 samples.\n",
    "   \n",
    "2. **PCA Colour Augmentation**: PCA was performed on the set of RBG values throughout the training set. The found principal components were first scaled by a magnitude proportional to the corresponding eigenvalues times a random variable drawn from a Gaussian with mean zero and standard deviation 0.1, then added to the training images. The desired effect here was that object identity be invariant to intensity and colour of illumination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image was cropped to a size of 256x256 (in order to ensure fixed sized input data), then from each pixel channel (RBG), the mean channel value over the entire dataset was subtracted from that channel. For Tiny Imagenet, we found the means to be [142, 143, 145] for Red, Green, and Blue respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Return a one-hot encoding (of size 200) for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(index):\n",
    "    '''\n",
    "    Args:\n",
    "        index: integer value in range 0-199\n",
    "    \n",
    "    Return:\n",
    "        vec: zero vector of size 200, with a 1 at the index position'''\n",
    "    \n",
    "    vec = np.zeros(200)\n",
    "    vec[index] = 1.0\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract all Wordnet IDs from Tiny Imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wnids(folder_path):\n",
    "    '''\n",
    "    Args:\n",
    "        folder_path: \"tiny-imagenet-200/train\n",
    "    \n",
    "    Return:\n",
    "        wnids: list of wnids in folder\n",
    "    '''\n",
    "    wnids = os.listdir(folder_path)\n",
    "    return wnids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rescale (using nearest neighbour alg) to size 224x224 and subtract channel means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "    '''\n",
    "    Args: \n",
    "        image: Image object\n",
    "    \n",
    "    Returns:\n",
    "        image: a resized 224x224x3 numpy array with R, B, and G means subtracted from respective channels\n",
    "    '''\n",
    "    means = np.array([142, 143, 145], np.int32)\n",
    "    image = np.array(image.resize((224, 224)), np.int32)\n",
    "    for i in range(3):\n",
    "        image[:,:,i] -= means[i]\n",
    "    \n",
    "    \n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomly select an image from a given class folder and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(images_folder):\n",
    "    '''\n",
    "    Args:\n",
    "        image_folder: path to image\n",
    "        \n",
    "    Returns:\n",
    "        image: randomly selected image and preprocess\n",
    "    '''\n",
    "    \n",
    "    flag = False\n",
    "    while flag is False:\n",
    "        image_path = os.path.join(images_folder, random.choice(os.listdir(images_folder)))\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        if np.array(img).shape == (64, 64, 3):  # ignore grayscale images\n",
    "            flag = True\n",
    "            \n",
    "    # load and normalize image\n",
    "    img_array = preprocess(img)\n",
    "    \n",
    "    return img_array    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomly select a class folder, extract/preprocess image, get one-hot encoded class, return batch size of choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_batch(batch_size, images_source, wnid_labels):\n",
    "    '''\n",
    "    Args:\n",
    "        batch_size: desired batch_size\n",
    "        images_source: \"tiny-imagenet-200/train\"\n",
    "    '''\n",
    "    batch_images = []\n",
    "    batch_labels = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # random class choice \n",
    "        # (randomly choose a folder of image of the same class from a list of previously sorted wnids)\n",
    "        class_index = random.randint(0, 199)\n",
    "\n",
    "        flag = False\n",
    "        while flag is False:\n",
    "            folder = os.path.join(wnid_labels[class_index], \"images\")\n",
    "            if os.path.exists(os.path.join(images_source,folder)):\n",
    "                flag = True\n",
    "        \n",
    "        batch_images.append(read_image(os.path.join(images_source, folder)))\n",
    "        batch_labels.append(one_hot(class_index))\n",
    "\n",
    "    np.vstack(batch_images)\n",
    "    np.vstack(batch_labels)\n",
    "    return batch_images, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnids = get_wnids(\"tiny-imagenet-200/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_batch(1, \"tiny-imagenet-200/train\", wnids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alexnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLU Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Krizhevsky et al replaced the standard neuron activation $ f(x) = tanh(x) $ in favour of the non-saturating (i.e. $ \\lim_{{x \\rightarrow \\infty}} f(x) = \\infty$) Rectified Linear Unit (ReLU) function. The work showed that deep CNNs with ReLU activations trained several times faster than their counterparts with tanh units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local Response Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After kernel $i$ is applied at position $(x, y)$ and passed through the ReLU, the neuron activity is normalized by the sum over $n$ adjacent kernel maps at the same spatial position. Essentially \"brightness normalization\", where peaks are enlarged and flat regions dampened. Reduced top-1 and top-5 errr rates by 1.4% and 1.2% respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overlapping Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas traditional pooling units were placed adjacently, without overlap, the Alexnet pooling units were set to overlap by a 1-pixel margin. Reduced top-1 and top-5 error rates by 0.4% and 0.3% respectively (compared to the non-overlapping scheme)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each neuron in the layer, we set a probability of 0.5 of setting the output of that neuron to zero. The neuron activity will not contribute to the forward pass or backpropogation. Reduces complex co-adaptations of neurons, forcing the learning of more robust image features. Roughly doubles the number of iterations required to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper Functions for TF Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight(shape, name):\n",
    "    init = tf.truncated_normal(shape, stddev=0.1)\n",
    "    w = tf.Variable(init, name=name)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias(val, shape, name):\n",
    "    init = tf.constant(val, shape=shape)\n",
    "    return tf.Variable(init, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(x, w, stride, padding=\"SAME\"):\n",
    "    return tf.nn.conv2d(x, w, strides=[1, stride, stride, 1], padding=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool(x, kernel, stride, padding=\"VALID\"):\n",
    "    return tf.nn.max_pool(x, ksize=[1, kernel, kernel, 1], strides=[1, stride, stride, 1], padding=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrn(x, radius=2, bias=1.0, alpha=2e-05, beta=0.75):\n",
    "        return tf.nn.local_response_normalization(x, depth_radius=radius, alpha=alpha, beta=beta, bias=bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return tf.nn.softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have to add weight decay (0.00005)\n",
    "dropout = 0.5\n",
    "momentum = 0.9\n",
    "lmbda = 5e-04\n",
    "learning_rate = 0.001\n",
    "epochs = 1\n",
    "batch_size = 128\n",
    "display_step = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 224, 224, 3], name=\"x\")\n",
    "y = tf.placeholder(tf.float32, [None, 200], name=\"y\")\n",
    "# lr = tf.placeholder(tf.float32)\n",
    "keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer 1: Convolution $ \\rightarrow $ Max-Pooling $ \\rightarrow $ Local Response Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_w1 = weight([11, 11, 3, 96], name=\"cnn_w1\")\n",
    "cnn_b1 = bias(0.0, [96], name=\"cnn_b1\")\n",
    "conv1 = tf.add(conv(x, cnn_w1, stride=4, padding=\"SAME\"), cnn_b1)\n",
    "conv1 = relu(conv1)\n",
    "pool1 = max_pool(conv1, kernel=3, stride=2)\n",
    "norm1 = lrn(pool1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer 2: Convolution $ \\rightarrow $ Max-Pooling $ \\rightarrow $ Local Response Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_w2 = weight([5, 5, 96, 256], name=\"cnn_w2\")\n",
    "cnn_b2 = bias(0.0, [256], name=\"cnn_b2\")\n",
    "conv2 = tf.add(conv(norm1, cnn_w2, stride=1, padding=\"SAME\"), cnn_b2)\n",
    "conv2 = relu(conv2)\n",
    "pool2 = max_pool(conv2, kernel=3, stride=2)\n",
    "norm2 = lrn(pool2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer 3: Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_w3 = weight([3, 3, 256, 384], name=\"cnn_w3\")\n",
    "cnn_b3 = bias(0.0, [384], name=\"cnn_b3\")\n",
    "conv3 = tf.add(conv(norm2, cnn_w3, stride=1, padding=\"SAME\"), cnn_b3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer 4: Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_w4 = weight([3, 3, 384, 384], name=\"cnn_w4\")\n",
    "cnn_b4 = bias(0.0, [384], name=\"cnn_b4\")\n",
    "conv4 = tf.add(conv(conv3, cnn_w4, stride=1, padding=\"SAME\"), cnn_b4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer 5: Convolution $\\rightarrow$ Max-Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_w5 = weight([3, 3, 384, 256], name=\"cnn_w5\")\n",
    "cnn_b5 = bias(0.0, [256], name=\"cnn_b5\")\n",
    "conv5 = tf.add(conv(conv4, cnn_w5, stride=1, padding=\"SAME\"), cnn_b5)\n",
    "pool5 = max_pool(conv5, kernel=3, stride=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = pool5.get_shape().as_list()\n",
    "flattened_dim = dim[1] * dim[2] * dim[3]  \n",
    "flattened = tf.reshape(pool5, [-1, flattened_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer 6: Fully-Connected with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_w1 = weight([flattened_dim, 4096], name=\"fc_w1\")\n",
    "fc_b1 = bias(0.0, [4096], name=\"fc_b1\")\n",
    "fc1 = tf.add(tf.matmul(flattened, fc_w1), fc_b1)\n",
    "fc1 = relu(fc1)\n",
    "fc1 = tf.nn.dropout(fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer 7: Fully-Connected with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_w2 = weight([4096, 4096], name=\"fc_w2\")\n",
    "fc_b2 = bias(0.0, [4096], name=\"fc_b2\")\n",
    "fc2 = tf.add(tf.matmul(fc1, fc_w2), fc_b2)\n",
    "fc2 = relu(fc2)\n",
    "fc2 = tf.nn.dropout(fc2, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer 8: Fully-Connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_w3 = weight([4096, 200], name=\"fc_w3\")\n",
    "fc_b3 = bias(0.0, [200], name=\"fc_b3\")\n",
    "pred = tf.add(tf.matmul(fc2, fc_w3), fc_b3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred, name=\"cross-entropy\"))\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=momentum, name=\"optimizer\").minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(pred,1), tf.argmax(y,1), name=\"correct_prediction\")\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"accuracy\")\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()\n",
    "for i in range(1):\n",
    "    batch = read_batch(1, \"tiny-imagenet-200/train\", wnids)\n",
    "    if i%1 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch[0], y: batch[1], keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "    optimizer.run(feed_dict={x: batch[0], y: batch[1], keep_prob: 0.5})\n",
    "\n",
    "test_batch = read_batch(5, \"tiny-imagenet-200/train\", wnids)\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x: test_batch[0], y: test_batch[1], keep_prob: 1.0}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
